<!DOCTYPE html>
<html>
<title>Defense Against the Dark Arts Page</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
<style>
body,h1,h2,h3,h4,h5 {font-family: "Raleway", sans-serif}
</style>
<body class="w3-light-grey">

<div class="w3-content" style="max-width:1400px">

<header class="w3-container w3-center w3-padding-32"> 
  <h1><b>Week 7 Write Up</b></h1>
  <p>Welcome to the page of <span class="w3-tag">Alexandra Carper</span></p>
</header>

<!-- Grid -->
<div class="w3-row">

<!-- Blog entries -->
<div class="w3-col l8 s12">
  <!-- Blog entry -->
  <hr>

  <!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Homework Write Up</b></h3>
    </div>

    <div class="w3-container">
      <p></p>
      
<div class="w3-container">
      <h5>Week 7 <span class="w3-opacity">August 11, 2019</span></h5>
	<p>	The topic under exploration this week was on web security, including web-centric attack vectors, modern web user agents, web research tools and attacks, server-side and client-side threat defense, and advanced content-agnostic research techniques. Within this exploration, we can attempt to look at these concepts from the angle of the bad guy based on the idea that understanding what the attackers do gives you a chance to offer code that is immune to their attacks. </p>
  <p>	It was especially interesting to learn about Oregon’s computer crime statute, which identifies three main categories of computer crimes. The first is knowingly accessing or using a computer or network for the purpose of fraud; to obtain money, property or services; or to commit theft of proprietary information. The second is knowingly and without authorization altering, destroying, or damaging any computer, network, software, or data. The third category is knowingly and without authorization using or accessing a computer or network. The penalties for these crimes can be severe and amount to years in prison. </p>
  <p>	Looking at the web as a delivery mechanism, we know that the web is everywhere, along with the HTTP protocol and HTML. In addition, technology evolves very quickly; one of the first web browsers, Mosaic, was built in 1993, and the attacks that were made at that time are no longer relevant today. In the mid-90’s, generic phishing, popups, and script bombing were more prevalent. Generic phishing refers to copying the behavior of certain websites, but it was easily detectable because there were mistakes in almost every word. Script bombing makes an infinite loop of putting an archive within an archive, which makes the script dose itself. In the bronze age, SQL injection, cross-site scripting, and browser exploits were popular web-based malware delivery vectors. Once we reached the iron age, we saw more sophisticated phishing attacks like spear phishing, plugin exploits, such as with Shockwave or Flash, and customized browser exploits. Nowadays, we still have advanced spear phishing attacks, but we also have HTML5-based attacks as well. </p>
  <p>	The basic version of a web browser has three layers that communicate with each other. The top layer is the network protocol, which interacts with the rest of the world through HTTP requests. The next layer is the DOM, where the processing is done by the browser, and the last layer is the user interface (UI), which is everything that interacts with the user. In a 2.0 version of a web browser we also have all of these layers, but the JavaScript is more vertical than horizontal, interacting with every single layer. This way, JavaScript can interact more directly with the network, but there is mostly some kind of JavaScript involved in every malicious attack. </p>
  <p>	This brings us to the topic of injection points, as the malware must interact with what you see in the browser. Within the HTTP layer, the threat can interact on the network or directly on the host, which makes the browser under attack from two different angles. The RAW HTML can also be intercepted by hijacking and hooking WinInet or using an event trace on Windows. The threat can interact with the HTML through extensions, and it can interact with the JavaScript through a script engine, which can either be built-in or externalized. Finally, a threat can interact with de-obfuscated, or rendered, content, by trying to make it so we cannot make sense of what is going on by attacking lower layers. </p>
  <p>	Usually, the user is the most vulnerable to malicious attacks. For instance, attackers can easily use tactics like social engineering, which uses psychological manipulation of people into performing actions or divulging confidential information, to gather information, commit fraud, or gain access to a system. Users are such a good target for attacks because they are typically impatient, lazy, have self-proclaimed omniscience, and they will click almost anything. Attackers utilize these exploits to direct the user to malicious content, which can be executables, browser exploits, or malicious forms, through their own actions. The attackers typically direct the users to this content through phishing, SEO poisoning, fake AV, social media link insertion, forum link insertion, and malvertising. </p>
  <p>	Although we covered phishing in previous material it is important to note the vulnerabilities. URLs are not always delivered via email and valid SSL certificates do not always protect the user. Phishing is no longer just restricted to banking and financial anymore, and it is being kept alive by spear phishing, social media and snowshoe spam. In addition, phishing sites tend to be very short-lived, which makes them harder to identify with a reputation. In an SEO poisoning attack, the attacker uses search results, such as celebrities, pop culture, or world events, to lure the web-searching victim. The strategy here is gaming the search engine’s relevance rules using related content, massive inbound linking, and relevant inbound linking, and the site will end up redirecting to the malicious content. </p>
  <p>	Fake updates and fake AV prey on the user’s good intentions. Usually, the attacker will mimic operating system level UI components and well-known security brands. Attackers can write code to show static filenames fast within the output to mimic a scanning function a typical security tool would have. Little does the user know, the code may be delivering malware in the background. Social media also has the potential to allow malicious URL delivery and information gathering. Finally, in malvertising, the malicious actor uses advertising networks as a delivery mechanism for their code. This kind of delivery mechanism does not require any web site or hosting provider compromise, and it is very dangerous, with most end users implicitly trusting content on well known websites. In malvertising, the attacker seeds malicious ads that are very specific, targeting a certain population. The target then goes to the established website that shares metadata with an advertising network. The ad reaches the clean website, redirects to a malicious website, and the target is now infected. </p>
  <p>	Another method of attacking on the web is called a waterhole attack. It was scary to learn that developers have become a prime target of these kinds of attacks because of closed operating systems and application signing. Attackers can easily hack a resource, such as a forum like StackOverflow, that is known to be used by a community of developers. Once the attackers hack this location, they wait for the people to come to them, redirect you when you come, and ultimately infect you. </p>
  <p>	Common defenses for user attacks include URL/domain reputation systems, site certification services, client and gateway AV/AM, safe URL shorteners, content provider education, and end user education. URL and domain reputation systems offer real-time protection in a browser or network device with a search result link annotation. Site certification services, such as PCI or EMET, create additional hoops to jump through for ensuring the safety of data. Client and gateway AV/AM can act as a basic layer to catch anything that tries to get out of the browser. URL shorteners can attempt to tell you if a link is safe, indicating it is not pointing directly to any malware. In addition, there are strategies to educate your users, such as content provider education through banks and brand monitoring services, and end user education, which is an approach that would make the user the last layer of defense. </p>
  <p>	With browser-level attacks, the browser is attacked by many different angles, sometimes including local software. These attacks are most commonly a mix of a few attack techniques combines. To prevent these attacks, there are certain security features included within the modern web browser, including content security policy enforcement, URL scheme access rules, OS isolation/sandboxing, redirection restrictions, content handling/sniffing, disruptive script handling, built-in and plug-in URL reputation clients, and add-on AV/AM content monitoring. Content security policy enforcement, for example, includes the Same Origin Policy, which relates to the DOM, XMLHTTPRequest, Cookies, Flash, and Java. It is also possible for the browser to have several different kinds of exploits, both known and unknown. To utilize these exploits, attackers will typically use a multi-step process in which they lure a user to a site, exploit the browser, and download or execute the actual payload, which can be any malware, web-centric or not. </p>
  <p>	Another tactic attackers use for browser-level attacks is content or script obfuscation. With this method, the attacker can rename script variables to meaningless, short, difficult to read text, remove whitespace, or make a massive block of text. Then they use self-generating code executed to create “itself”, which is then executed. While we can use defenses like advanced text processing, browser plugin or proxy servers, or blocking content we deem malicious, attackers can also pull sneaky moves like using something specific to your endpoint so when you capture the same script somewhere else it does not behave the same way. Basically, this technique is like a big puzzle with code that can fetch from many different locations, reassemble it, render it after many loops, and get it to the final layer, which is most frequently your exploit. </p>
  <p>Man in the middle attacks, which was covered in last week’s material, can also be used as a browser-level attack. Attackers can intercept and modify traffic in real-time, but MITM requires the ability for mid-traffic insertion, which can be obtained through DNS poisoning, WiFi hacking, ARP attacks, or a rogue proxy. HTTP, or the hypertext transfer protocol, which is a text-based transport protocol, is very easy to intercept. HTTPS, on the other hand, is somewhat harder to intercept. Attackers can intercept HTTP or HTTPS to rewrite content in-line through within either the proxy or the packet. Man in the browser is the dangerous cousin to the MITM attack. MITB can intercept and modify traffic to/from the server, but inside of the browser. Attackers do this inside of the browser because escaping the browser may arouse suspicion. For example, attackers can change financial account numbers mid-stream or sniff and steal passwords. MITB attacks can use methods such as browser exploits, plug in exploits, or new HTML5 features such as web sockets and web workers to be successful. </p>
  <p>Two other methods of attack are DNS spoofing and clickjacking. In DNS spoofing, or DNS cache poisoning, the browser starts out by asking for a legitimate domain. The ISP DNS server returns a malicious IP instead, since it has a poison cache, and the browser fetches the content from the malicious IP using the correct host in the HTTP header. In clickjacking, the attacker tricks the user into clicking a pre-determined link in a rendered HTML page. The attacker will used HTML frames and layers to confuse the user, and they will hide malicious content behind legitimate content, such as the Facebook like button or the retweet button on Twitter. But this method is not just for clicks anymore, it can also be used for text input of passwords or personal information. </p>
  <p>SQL injection exists because most sites are database-driven, accept user data submissions via GET/POST, and manage authentication and authorization via the database. The fact that most web developers are not security experts adds fuel to the fire. Thus, attackers can circumvent authentication and authorization within a database to reveal private data. They can do this by crafting an SQL request so they can access information, injecting syntactically correct SQL code and adding something else that makes sense to get the information they want. There are many different types of SQL injection. For instance, there is error-driven SQL injection, which gets all the information from the error, numeric or string injection, SQL statement injection, user defined function injection, and blind injection, which can be timer based or condition based. SQL injection is not limited to data, however; most SQL servers offer a mechanism for bulk import and export from files, many provide mechanisms to query remote databases, and some even provide the ability to execute shell commands. Thus, SQL injection can be used for file system access, exfiltration, and execution. </p>
  <p>With Same Origin Policy attacks, there is a tradeoff. While the browser must keep non-related resources isolated, it must also allow for the modern web, where resources and APIs are commonly shared. SOP is a core security feature in all browsers, although it is not standardized, that is based on the principle that a, “resource from an origin may only access the same origin.” The origin referred to here is a site identifier that is keyed by the scheme, the host, and the port of the URI. It controls the DOM access between scripts running in the browser, XMLHttpRequest requests, cookie access, and plugin access. But the Internet would be very boring if we did not break these rules for images, linking, forms, and script sourcing. While SOP can stop a script from making requests to another origin whenever a user downloads a page with an included script from an origin, we can get around this using Cross Origin Resource Sharing. </p>
  <p>Cross Site Scripting allows us to bypass SOP rules and allow for script execution. The goal is to inject a client-side script into another user’s browser by injecting code and exploiting a user’s trust in a site. The malicious actor can post to a trusted website and embed a script in the submission data, which is accessed and displayed back with the script and the malicious content. The two categories of XSS include persistent, which is more dangerous, and non-persistent, which is more common. The evil brother to XSS is Cross-Site Request Forgery. This is the opposite of XSS, exploiting the server’s trust in the browser, not the browser’s trust in the server. The goal of XSRF is to execute the malicious actions against a user as that user on their trusted server. The typical sites that are targeted rely on identity, such as banking and finance sites, email, and forums or social media. The attackers will exploit the site’s trust in that identity, with the victim’s request pre-authenticated by a cookie. The user’s browser is tricked into sending requests to the target site, where side-effects such as transferring funds or changing passwords can be invoked. </p>
  <p>At this point, Cedric introduced us to his tool he developed to find SQL injection and make it easy to get something out of it. We also looked at the Tamper Data plugin, which allows you to see GET/POST queries with POSTDATA, intercept cookies, and tamper with queries. The web developer tools within the browser, or specifically Chrome, can benchmark the speed of a page, edit the contents of the page, or be used to inspect the password field element and modify the contents on the fly. </p>
  <p>There are several tools that were covered briefly within the malware toolbox. Alexa is a useful tool for determining general site popularity and prevalence. Data is collected via end-user toolbars, and it is domain-based. There are different thresholds for clean or infected data, but generally attackers get prevalent very quickly and then die, so we can infer the cleanliness of a site when it has been prevalent for a long time. Archive.org may be useful for determining site changes over time, but it is not useful for short-lived malware content. IPVOID is able to check an IP against a large list of IP blacklists. There are more than 35 blacklists that are anti-spam, anti-phishing, and anti-malware. We can use this tool to gain clues about how malicious a URL and domain for that URL is, as IPs are not easy to come by, and a bad domain most frequently has one IP. </p>
  <p>Sinkholes can be used to see the size of a botnet, the endpoints and how many people were infected by malware. Sinkholing a domain is based on the idea that a lot of malware uses botnets that call back home and want to know what to do. CheckShortUrl is a URL expander service for most short URL services. There is a single clearinghouse, and many services are supported at this single location. This tool can expand the URL to see where it points to. Site Dossier is able to provide general site information, such as IP, parent, DNS servers, and inbound links. This tool can give you a bigger picture about the URL, the domain, and what they point to. Webutation is a URL reputation clearinghouse that can filter based on web reputation. Web Inspector is an online web scanning tool that also provides a list of recently detected malicious sites. The classification techniques it uses are blacklist membership, AV scanning, static analysis, and dynamic analysis. Virus total, another online web scanning tool, is similar to this, also providing a list of malware files and using the same classification techniques. </p>
  <p>LinuxJWhoIs is a domain registration data client. For a given name, it can provide information about the registrar, registration data, expiration date, and contact name. While there is no standard result format and responses can be unreliable or throttled, it can also be very telling. Linux DIG is a DNS resolver utility. For a given domain, it can provide information about SOA, A records, MX records, CNAME records, NS records, AAAA records, and TXT records. In contrast to Linux JWhoIs, Linux DIG provides standard result format. Indicators of Compromise, or threat feeds, connects to the DOTs and provides contextual data around different malicious objects, such as URL/domain, IP/ASN, the file hash, and the actor. </p>
  <p>The client-side web research tools used to analyze web malware are PhantomJS, Burp Suite, Web Scarab, JSUnpack, and Firebug. PhantomJS is a scriptable, headless, Webkit browser that executes all scripts and fully renders the page. The tool, which is a very active open-source project, has full DOM support and page thumbnail dump support and can be driven by many scripting languages. It allows for interaction with the page’s JavaScript, and it accepts user-defined handlers and callbacks. JSUnpack is an open-source command-line utility. It detects exploits that target browser and browser plugin vulnerabilities and deobfuscates and unpacks JavaScript. The input supported by this tool includes PDF files, packet captures, HTML files, JavaScript files and SWF files. Burp Suite can be used to intercept and modify traffic to and from the remote website. It can also log resource request and perform spidering, which is following links from one URL to the entire web. WebScarab can also spider, and it is also used to intercept and modify requests. In addition, it provides submission parameter fuzzing and session analysis. Finally, firebug allows the user to inspect HTML elements, explore the page script, modify the DOM and set breakpoints. </p>
  <p>The last topic under focus is URL classification. There are many types of URL classification, such as content-based classification, host/lexical classification, lexical-based classification, and graph-based URL classification. Content-based classification can be manual, static, low-interaction, or high-interaction. Manual involves using web research tools, a secure environment, and your brain to classify the URL. Static classification involves using automated methods, looking at the page content for malicious features but not executing the content. Low-interaction involves using tools such as PhantomJS to automatically render and execute content, noting the content’s features. High-interaction classification involves using a sandboxed or virtual environment to render and execute content with a real browser, letting the full operating system be compromised. This technique is especially useful for zero-day attacks. </p>
  <p>Host/lexical classification classifies a URL without access to its content, as content may not be available because of single-shot malware, auto-cloaking malware, or short-lived malware. Crawling billions of URLs is very expensive, and we want to classify URLs before any malicious content is served. This kind of classification is performed through human-authored rules, machine learning, or a combination of both. Lexical-based classification extracts features from the URL string to classify, including the domain, the path, CGI, and other features. The domain, for instance, can give us information about the length, ratio of alpha to numeric characters, IDN encoding, number of subdomains, ratio of domain tokens to dictionary words, and TLD. Graph-based URL classification utilize graph database engines to traverse a directed graph quickly and efficiently and store very large data sets. This classification technique is based on “guilt by association”, assuming an endpoint is infected if it points to another endpoint that is infected. </p>
  
 </div>
  </div>
</div>
<div class="w3-col l4">
  <div class="w3-card w3-margin w3-margin-top">
    <div class="w3-container w3-white">
      <h4><b>Alexandra Carper</b></h4>
      <p>This page was created for Weekly Writeups for Defense Against the Dark Arts</p>
    </div>
  </div>
 
  

</div>

</div><br>

</div>

<footer>
</footer>

</body>
</html>
